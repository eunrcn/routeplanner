{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional data to set up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = { # k:v. original state: [lst of possible next states]\n",
    "            \"S1\": [\"S2\", \"S6\", \"S10\"],\n",
    "            \"S2\": [\"S1\", \"S6\", \"S7\", \"S3\", \"S11\"],\n",
    "            \"S3\": [\"S2\", \"S7\", \"S8\", \"S4\"],\n",
    "            \"S4\": [\"S3\", \"S8\", \"S9\", \"S5\"],\n",
    "            \"S5\": [\"S4\", \"S9\", \"S14\"],\n",
    "            \"S6\": [\"S1\", \"S2\", \"S10\", \"S11\", \"S7\"],\n",
    "            \"S7\": [\"S2\", \"S3\", \"S8\", \"S12\", \"S11\", \"S6\"],\n",
    "            \"S8\": [\"S3\", \"S4\", \"S7\", \"S9\", \"S12\", \"S13\"],\n",
    "            \"S9\": [\"S4\", \"S5\", \"S8\", \"S13\", \"S14\"],\n",
    "            \"S10\": [\"S1\", \"S6\", \"S11\"],\n",
    "            \"S11\": [\"S10\", \"S6\", \"S2\", \"S7\", \"S12\"],\n",
    "            \"S12\": [\"S11\", \"S7\", \"S8\", \"S13\"],\n",
    "            \"S13\": [\"S12\", \"S8\", \"S9\", \"S14\"],\n",
    "            \"S14\": [\"S13\", \"S9\", \"S5\"]\n",
    "        }\n",
    "\n",
    "################################################################################################\n",
    "# Add weather score to states \n",
    "################################################################################################\n",
    "\n",
    "# Weather exposure lookup dictionary\n",
    "weather_exposure = {\n",
    "    (1, 2): 95, (1, 6): 1, (1, 10): 1,\n",
    "    (2, 1): 95, (2, 6): 5, (2, 7): 90, (2, 3): 44, (2, 11): 32,\n",
    "    (3, 2): 44, (3, 7): 57, (3, 8): 9, (3, 4): 41,\n",
    "    (4, 3): 41, (4, 8): 54, (4, 9): 50, (4, 5): 65,\n",
    "    (5, 4): 65, (5, 9): 9, (5, 14): 52,\n",
    "    (6, 1): 1, (6, 2): 5, (6, 10): 27, (6, 11): 50, (6, 7): 82,\n",
    "    (7, 2): 90, (7, 3): 57, (7, 8): 51, (7, 12): 33, (7, 11): 27, (7, 6): 82,\n",
    "    (8, 3): 9, (8, 4): 54, (8, 7): 51, (8, 9): 84, (8, 12): 36, (8, 13): 22,\n",
    "    (9, 4): 50, (9, 5): 9, (9, 8): 84, (9, 13): 31, (9, 14): 39,\n",
    "    (10, 1): 1, (10, 6): 27, (10, 11): 52,\n",
    "    (11, 10): 52, (11, 6): 50, (11, 2): 32, (11, 7): 27, (11, 12): 18,\n",
    "    (12, 11): 18, (12, 7): 33, (12, 8): 36, (12, 13): 46,\n",
    "    (13, 12): 46, (13, 8): 22, (13, 9): 31, (13, 14): 80,\n",
    "    (14, 13): 80, (14, 9): 39, (14, 5): 52\n",
    "}\n",
    "\n",
    "# Assign weather exposure scores for each action\n",
    "weather_scores = {\n",
    "    state: {action: weather_exposure.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in possible_actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for weather score\n",
    "# define the state and action to retrieve the weather score\n",
    "state, action = \"S3\", \"S8\"\n",
    "weather_score = weather_scores[state][action]\n",
    "\n",
    "################################################################################################\n",
    "# Add safety score to states\n",
    "################################################################################################\n",
    "\n",
    "# Safety exposure lookup dictionary\n",
    "safety_exposure = {\n",
    "    (1, 2): 22, (1, 6): 13, (1, 10): 15,\n",
    "    (2, 1): 22, (2, 6): 19, (2, 7): 36, (2, 3): 41, (2,11): 21,\n",
    "    (3, 2): 41, (3, 7): 36, (3, 8): 42, (3, 4): 48,\n",
    "    (4, 3): 48, (4, 8): 42, (4, 9): 62, (4, 5): 66,\n",
    "    (5, 4): 66, (5, 9): 99, (5, 14): 99,\n",
    "    (6, 1): 13, (6, 2): 19, (6, 10): 0, (6, 11): 18, (6, 7): 21,\n",
    "    (7, 2): 36, (7, 3): 36, (7, 8): 38, (7, 12): 39, (7, 11): 18, (7, 6): 21,\n",
    "    (8, 3): 42, (8, 4): 42, (8, 7): 38, (8, 9): 64, (8, 12): 58, (8, 13): 69,\n",
    "    (9, 4): 62, (9, 5): 99, (9, 8): 64, (9, 13): 58, (9, 14): 99,\n",
    "    (10, 1): 15, (10, 6): 0, (10, 11): 1,\n",
    "    (11, 10): 1, (11, 6): 18, (11, 2): 21, (11, 7): 18, (11, 12): 46,\n",
    "    (12, 11): 46, (12, 7): 39, (12, 8): 58, (12, 13): 69,\n",
    "    (13, 12): 69, (13, 8): 69, (13, 9): 58, (13, 14): 99,\n",
    "    (14, 13): 99, (14, 9): 99, (14, 5): 99\n",
    "}\n",
    "\n",
    "# Assign safety scores for each action\n",
    "safety_scores = {\n",
    "    state: {action: safety_exposure.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in possible_actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for safety score\n",
    "# define the state and action to retrieve the safety score\n",
    "state, action = \"S3\", \"S8\"\n",
    "safety_score = safety_scores[state][action]\n",
    "\n",
    "################################################################################################\n",
    "# Add travel time score to states\n",
    "################################################################################################\n",
    "\n",
    "# Travel Time lookup dictionary\n",
    "travel_time = {\n",
    "    (1, 2): 20, (1, 6): 80, (1, 10): 100,\n",
    "    (2, 1): 20, (2, 6): 60, (2, 7): 60, (2, 3): 40, (2, 11): 100,\n",
    "    (3, 2): 40, (3, 7): 60, (3, 8): 60, (3, 4): 20,\n",
    "    (4, 3): 20, (4, 8): 60, (4, 9): 60, (4, 5): 20,\n",
    "    (5, 4): 20, (5, 9): 60, (5, 14): 100,\n",
    "    (6, 1): 80, (6, 2): 60, (6, 10): 60, (6, 11): 40, (6, 7): 20,\n",
    "    (7, 2): 60, (7, 3): 60, (7, 8): 20, (7, 12): 60, (7, 11): 60, (7, 6): 20,\n",
    "    (8, 3): 60, (8, 4): 60, (8, 7): 20, (8, 9): 20, (8, 12): 60, (8, 13): 40,\n",
    "    (9, 4): 60, (9, 5): 60, (9, 8): 20, (9, 13): 60, (9, 14): 60,\n",
    "    (10, 1): 100, (10, 6): 60, (10, 11): 20,\n",
    "    (11, 10): 20, (11, 6): 40, (11, 2): 100, (11, 7): 60, (11, 12): 20,\n",
    "    (12, 11): 20, (12, 7): 60, (12, 8): 60, (12, 13): 20,\n",
    "    (13, 12): 20, (13, 8): 40, (13, 9): 60, (13, 14): 20,\n",
    "    (14, 13): 20, (14, 9): 60, (14, 5): 100\n",
    "}\n",
    "\n",
    "# Assign travel time scores for each action\n",
    "travel_time_scores = {\n",
    "    state: {action: travel_time.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in possible_actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for travel time score\n",
    "# define the state and action to retrieve the travel time score\n",
    "state, action = \"S3\", \"S8\"\n",
    "travel_time_score = travel_time_scores[state][action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up our environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestrianPaths:\n",
    "    def __init__(self, weather, safety, travel):\n",
    "        self.state = \"S1\"  # Start state\n",
    "        self.goal = \"S14\"\n",
    "        self.num_states = 14\n",
    "        self.possible_actions = { # k:v. original state: [lst of possible next states]\n",
    "            \"S1\": [\"S2\", \"S6\", \"S10\"],\n",
    "            \"S2\": [\"S1\", \"S6\", \"S7\", \"S3\", \"S11\"],\n",
    "            \"S3\": [\"S2\", \"S7\", \"S8\", \"S4\"],\n",
    "            \"S4\": [\"S3\", \"S8\", \"S9\", \"S5\"],\n",
    "            \"S5\": [\"S4\", \"S9\", \"S14\"],\n",
    "            \"S6\": [\"S1\", \"S2\", \"S10\", \"S11\", \"S7\"],\n",
    "            \"S7\": [\"S2\", \"S3\", \"S8\", \"S12\", \"S11\", \"S6\"],\n",
    "            \"S8\": [\"S3\", \"S4\", \"S7\", \"S9\", \"S12\", \"S13\"],\n",
    "            \"S9\": [\"S4\", \"S5\", \"S8\", \"S13\", \"S14\"],\n",
    "            \"S10\": [\"S1\", \"S6\", \"S11\"],\n",
    "            \"S11\": [\"S10\", \"S6\", \"S2\", \"S7\", \"S12\"],\n",
    "            \"S12\": [\"S11\", \"S7\", \"S8\", \"S13\"],\n",
    "            \"S13\": [\"S12\", \"S8\", \"S9\", \"S14\"],\n",
    "            \"S14\": [\"S13\", \"S9\", \"S5\"]\n",
    "        }\n",
    "        self.transition_probabilities = {\n",
    "            \"S1\": 0.6,\n",
    "            \"S2\": 0.6,\n",
    "            \"S3\": 0.6,\n",
    "            \"S4\": 0.6,\n",
    "            \"S5\": 0.6,\n",
    "            \"S6\": 0.7,\n",
    "            \"S7\": 0.7,\n",
    "            \"S8\": 0.7,\n",
    "            \"S9\": 0.7,\n",
    "            \"S10\": 0.7,\n",
    "            \"S11\": 0.8,\n",
    "            \"S12\": 0.8,\n",
    "            \"S13\": 0.9,\n",
    "            \"S14\": 0.9\n",
    "        }\n",
    "        self.weather = weather\n",
    "        self.safety = safety\n",
    "        self.travel = travel\n",
    "        # Define the normalization ranges (assuming max and min possible values for each score type)\n",
    "        self.WEATHER_MAX = 1000\n",
    "        self.SAFETY_MAX = 1000\n",
    "        self.TRAVEL_TIME_MAX = 1000\n",
    "        # Define the weights for each score (adjust based on their importance)\n",
    "        self.WEATHER_WEIGHT = 0.3\n",
    "        self.SAFETY_WEIGHT = 0.4\n",
    "        self.TRAVEL_TIME_WEIGHT = 0.3\n",
    "\n",
    "    def step(self, action): #str -> (str, int, bool) #simple since deterministic action\n",
    "        \"\"\" Move in the environment based on action \"\"\"\n",
    "        probability = self.transition_probabilities[action]\n",
    "        if random.random() < probability: #successfully found the way to this state\n",
    "            self.state = action\n",
    "        #else, got lost and remain in the same state\n",
    "        \n",
    "        # Define rewards\n",
    "        reward = 1 if self.state == self.goal else -0.3\n",
    "        done = self.state == self.goal  # Episode ends when goal is reached\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def reset(self): # _ -> str\n",
    "        self.state = \"S1\"\n",
    "        return self.state\n",
    "    \n",
    "################################################################################################\n",
    "# Functions to dynamically calculate the reward based on the state and action, using the scores (also based on the (weather, safety, travel time) weights depending on what user prioritizes most?)\n",
    "################################################################################################\n",
    "    def normalize_score(self, score, max_score): #int,int -> int\n",
    "        \"\"\" Normalize the score based on the maximum score for each category. \"\"\"\n",
    "        return score / max_score\n",
    "\n",
    "\n",
    "    def calculate_reward(self, state, action, weather_weight, safety_weight, travel_time_weight): # -> int\n",
    "        \"\"\" \n",
    "        Smarter reward function that considers normalized scores and weighted importance. \n",
    "        Incorporates dynamic scaling for weather, safety, and travel time.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve individual scores\n",
    "        weather_score = self.weather_scores[state][action]\n",
    "        safety_score = self.safety_scores[state][action]\n",
    "        travel_time_score = self.travel_time_scores[state][action]\n",
    "\n",
    "        # Normalize the scores to be between 0 and 1\n",
    "        normalized_weather = self.normalize_score(weather_score, self.WEATHER_MAX)\n",
    "        normalized_safety = self.normalize_score(safety_score, self.SAFETY_MAX)\n",
    "        normalized_travel_time = self.normalize_score(\n",
    "            travel_time_score, self.TRAVEL_TIME_MAX)\n",
    "\n",
    "        # Calculate weighted sum of normalized scores\n",
    "        total_score = (self.WEATHER_WEIGHT * normalized_weather +\n",
    "                    self.SAFETY_WEIGHT * normalized_safety +\n",
    "                    self.TRAVEL_TIME_WEIGHT * normalized_travel_time)\n",
    "\n",
    "        # Reward is the negative of the total score (since higher scores should be worse)\n",
    "        reward = -1 * total_score\n",
    "\n",
    "        return reward\n",
    "\n",
    "env = PedestrianPaths(weather_scores, safety_scores, travel_time_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up our RL agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 14\n",
    "action_size = 14\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, state_size, action_size, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = np.full((state_size, action_size), float('-inf'))  # Q-value table instantiate all as impossible actions first\n",
    "        self.env = env\n",
    "\n",
    "        #certain values in the q_table for select few states that can be reached from each state\n",
    "        curr_state = 0\n",
    "        for lst_next_state in env.possible_actions.values():\n",
    "            for next_state in lst_next_state:\n",
    "                state_index = int(next_state[1:])-1\n",
    "                self.q_table[curr_state][state_index] = 0 #instantiate to 0 for all possible actions\n",
    "            curr_state+=1\n",
    "            if curr_state == self.state_size:\n",
    "                break\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\" Epsilon-greedy policy for action selection \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.env.possible_actions[state]) # Explore\n",
    "        else:\n",
    "            state_index = int(state[1:])-1\n",
    "            return \"S\" + str(np.argmax(self.q_table[state_index])+1)  # Exploit\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\" Q-Learning update rule (Off-policy TD control) \"\"\"\n",
    "        state_index = int(state[1:])-1\n",
    "        action_index = int(action[1:])-1\n",
    "        next_state_index = int(next_state[1:])-1\n",
    "        best_next_action_index = np.argmax(self.q_table[next_state_index])  # Off-policy: Use max Q-value\n",
    "        td_target = reward + self.gamma * self.q_table[next_state_index, best_next_action_index]\n",
    "        td_error = td_target - self.q_table[state_index, action_index]\n",
    "        self.q_table[state_index, action_index] += self.alpha * td_error  # Update rule\n",
    "\n",
    "agent = QLearningAgent(env, state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       -inf -0.63829747        -inf        -inf        -inf -0.55727319\n",
      "         -inf        -inf        -inf -0.68472307        -inf        -inf\n",
      "         -inf        -inf]\n",
      " [-0.43582268        -inf -0.39738375        -inf        -inf -0.35880283\n",
      "  -0.32603219        -inf        -inf        -inf -0.17410456        -inf\n",
      "         -inf        -inf]\n",
      " [       -inf -0.19539432        -inf -0.16163023        -inf        -inf\n",
      "  -0.17561806 -0.17672629        -inf        -inf        -inf        -inf\n",
      "         -inf        -inf]\n",
      " [       -inf        -inf -0.0944436         -inf -0.05997           -inf\n",
      "         -inf -0.05997     0.19516457        -inf        -inf        -inf\n",
      "         -inf        -inf]\n",
      " [       -inf        -inf        -inf -0.02377742        -inf        -inf\n",
      "         -inf        -inf  0.                -inf        -inf        -inf\n",
      "         -inf  0.        ]\n",
      " [-0.63216356 -0.4477124         -inf        -inf        -inf        -inf\n",
      "  -0.32568689        -inf        -inf -0.47297236 -0.09282217        -inf\n",
      "         -inf        -inf]\n",
      " [       -inf -0.24463735 -0.24273943        -inf        -inf -0.25219775\n",
      "         -inf -0.2234861         -inf        -inf -0.2343931   0.28429231\n",
      "         -inf        -inf]\n",
      " [       -inf        -inf -0.10264385 -0.11299999        -inf        -inf\n",
      "  -0.09706416        -inf  0.04128875        -inf        -inf  0.28281481\n",
      "   0.11765435        -inf]\n",
      " [       -inf        -inf        -inf -0.09058586 -0.03              -inf\n",
      "         -inf  0.                -inf        -inf        -inf        -inf\n",
      "   0.03233185  0.93752438]\n",
      " [-0.6184917         -inf        -inf        -inf        -inf -0.45531312\n",
      "         -inf        -inf        -inf        -inf -0.09462762        -inf\n",
      "         -inf        -inf]\n",
      " [       -inf -0.29416603        -inf        -inf        -inf -0.28002682\n",
      "  -0.09736205        -inf        -inf -0.32760722        -inf  0.27195846\n",
      "         -inf        -inf]\n",
      " [       -inf        -inf        -inf        -inf        -inf        -inf\n",
      "   0.03878107  0.01600809        -inf        -inf -0.02606087        -inf\n",
      "   0.66935152        -inf]\n",
      " [       -inf        -inf        -inf        -inf        -inf        -inf\n",
      "         -inf  0.05425332  0.57645814        -inf        -inf  0.41349923\n",
      "         -inf  0.97468305]\n",
      " [       -inf        -inf        -inf        -inf  0.                -inf\n",
      "         -inf        -inf  0.                -inf        -inf        -inf\n",
      "   0.                -inf]]\n"
     ]
    }
   ],
   "source": [
    "env = PedestrianPaths(weather_scores, safety_scores, travel_time_scores)\n",
    "agent = QLearningAgent(env, state_size, action_size)\n",
    "\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Update Q-table using Q-learning\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Print learned Q-values\n",
    "print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
