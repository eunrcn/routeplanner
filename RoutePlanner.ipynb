{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for state S3 and action S8: -0.037500000000000006\n",
      "Optimal Route: ['S1', np.str_('S3'), np.str_('S11'), np.str_('S14')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RoutePlannerMDP:\n",
    "    def __init__(self, states, actions, transition_probabilities, rewards, gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.P = transition_probabilities\n",
    "        self.R = rewards  # rewards is now a dictionary of dictionaries\n",
    "        self.gamma = gamma\n",
    "        self.policy = {s: np.random.choice(actions[s]) for s in states}\n",
    "\n",
    "    def value_iteration(self, theta=1e-6):\n",
    "        V = {s: 0 for s in self.states}\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = V[s]\n",
    "                max_value = float('-inf')\n",
    "                for a in self.actions[s]:\n",
    "                    action_value = sum(self.P[s][a][s_next] *\n",
    "                                       (self.R[s][a] + self.gamma * V[s_next])\n",
    "                                       for s_next in self.P[s][a])\n",
    "                    max_value = max(max_value, action_value)\n",
    "                V[s] = max_value\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        for s in self.states:\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for a in self.actions[s]:\n",
    "                action_value = sum(self.P[s][a][s_next] *\n",
    "                                   (self.R[s][a] + self.gamma * V[s_next])\n",
    "                                   for s_next in self.P[s][a])\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = a\n",
    "            self.policy[s] = best_action\n",
    "\n",
    "\n",
    "    def get_optimal_route(self, start_state, max_steps=100):\n",
    "        route = [start_state]\n",
    "        current_state = start_state\n",
    "        steps = 0\n",
    "        while current_state != \"S14\" and steps < max_steps:\n",
    "            action = self.policy[current_state]\n",
    "            next_states = list(self.P[current_state][action].keys())\n",
    "            probabilities = list(self.P[current_state][action].values())\n",
    "            current_state = np.random.choice(next_states, p=probabilities)\n",
    "            route.append(current_state)\n",
    "            steps += 1\n",
    "        if current_state != \"S14\":\n",
    "            route.append(\"Max Steps Exceeded\")\n",
    "        return route\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Define states and actions\n",
    "states = [f\"S{i}\" for i in range(1, 15)]\n",
    "\n",
    "actions = {\n",
    "    \"S1\": [\"S2\", \"S6\", \"S10\"],\n",
    "    \"S2\": [\"S1\", \"S6\", \"S7\", \"S3\", \"S11\"],\n",
    "    \"S3\": [\"S2\", \"S7\", \"S8\", \"S4\"],\n",
    "    \"S4\": [\"S3\", \"S8\", \"S9\", \"S5\"],\n",
    "    \"S5\": [\"S4\", \"S9\", \"S14\"],\n",
    "    \"S6\": [\"S1\", \"S2\", \"S10\", \"S11\", \"S7\"],\n",
    "    \"S7\": [\"S2\", \"S3\", \"S8\", \"S12\", \"S11\", \"S6\"],\n",
    "    \"S8\": [\"S3\", \"S4\", \"S7\", \"S9\", \"S12\", \"S13\"],\n",
    "    \"S9\": [\"S4\", \"S5\", \"S8\", \"S13\", \"S14\"],\n",
    "    \"S10\": [\"S1\", \"S6\", \"S11\"],\n",
    "    \"S11\": [\"S10\", \"S6\", \"S2\", \"S7\", \"S12\"],\n",
    "    \"S12\": [\"S11\", \"S7\", \"S8\", \"S13\"],\n",
    "    \"S13\": [\"S12\", \"S8\", \"S9\", \"S14\"],\n",
    "    \"S14\": [\"S13\", \"S9\", \"S5\"]\n",
    "}\n",
    "\n",
    "################################################################################################\n",
    "# Add weather score to states \n",
    "################################################################################################\n",
    "\n",
    "# Weather exposure lookup dictionary\n",
    "weather_exposure = {\n",
    "    (1, 2): 95, (1, 6): 1, (1, 10): 1,\n",
    "    (2, 1): 95, (2, 6): 5, (2, 7): 90, (2, 3): 44, (2, 11): 32,\n",
    "    (3, 2): 44, (3, 7): 57, (3, 8): 9, (3, 4): 41,\n",
    "    (4, 3): 41, (4, 8): 54, (4, 9): 50, (4, 5): 65,\n",
    "    (5, 4): 65, (5, 9): 9, (5, 14): 52,\n",
    "    (6, 1): 1, (6, 2): 5, (6, 10): 27, (6, 11): 50, (6, 7): 82,\n",
    "    (7, 2): 90, (7, 3): 57, (7, 8): 51, (7, 12): 33, (7, 11): 27, (7, 6): 82,\n",
    "    (8, 3): 9, (8, 4): 54, (8, 7): 51, (8, 9): 84, (8, 12): 36, (8, 13): 22,\n",
    "    (9, 4): 50, (9, 5): 9, (9, 8): 84, (9, 13): 31, (9, 14): 39,\n",
    "    (10, 1): 1, (10, 6): 27, (10, 11): 52,\n",
    "    (11, 10): 52, (11, 6): 50, (11, 2): 32, (11, 7): 27, (11, 12): 18,\n",
    "    (12, 11): 18, (12, 7): 33, (12, 8): 36, (12, 13): 46,\n",
    "    (13, 12): 46, (13, 8): 22, (13, 9): 31, (13, 14): 80,\n",
    "    (14, 13): 80, (14, 9): 39, (14, 5): 52\n",
    "}\n",
    "\n",
    "# Assign weather exposure scores for each action\n",
    "weather_scores = {\n",
    "    state: {action: weather_exposure.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for weather score\n",
    "# define the state and action to retrieve the weather score\n",
    "state, action = \"S3\", \"S8\"\n",
    "weather_score = weather_scores[state][action]\n",
    "\n",
    "################################################################################################\n",
    "# Add safety score to states\n",
    "################################################################################################\n",
    "\n",
    "# Safety exposure lookup dictionary\n",
    "safety_exposure = {\n",
    "    (1, 2): 22, (1, 6): 13, (1, 10): 15,\n",
    "    (2, 1): 22, (2, 6): 19, (2, 7): 36, (2, 3): 41, (2,11): 21,\n",
    "    (3, 2): 41, (3, 7): 36, (3, 8): 42, (3, 4): 48,\n",
    "    (4, 3): 48, (4, 8): 42, (4, 9): 62, (4, 5): 66,\n",
    "    (5, 4): 66, (5, 9): 99, (5, 14): 99,\n",
    "    (6, 1): 13, (6, 2): 19, (6, 10): 0, (6, 11): 18, (6, 7): 21,\n",
    "    (7, 2): 36, (7, 3): 36, (7, 8): 38, (7, 12): 39, (7, 11): 18, (7, 6): 21,\n",
    "    (8, 3): 42, (8, 4): 42, (8, 7): 38, (8, 9): 64, (8, 12): 58, (8, 13): 69,\n",
    "    (9, 4): 62, (9, 5): 99, (9, 8): 64, (9, 13): 58, (9, 14): 99,\n",
    "    (10, 1): 15, (10, 6): 0, (10, 11): 1,\n",
    "    (11, 10): 1, (11, 6): 18, (11, 2): 21, (11, 7): 18, (11, 12): 46,\n",
    "    (12, 11): 46, (12, 7): 39, (12, 8): 58, (12, 13): 69,\n",
    "    (13, 12): 69, (13, 8): 69, (13, 9): 58, (13, 14): 99,\n",
    "    (14, 13): 99, (14, 9): 99, (14, 5): 99\n",
    "}\n",
    "\n",
    "# Assign safety scores for each action\n",
    "safety_scores = {\n",
    "    state: {action: safety_exposure.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for safety score\n",
    "# define the state and action to retrieve the safety score\n",
    "state, action = \"S3\", \"S8\"\n",
    "safety_score = safety_scores[state][action]\n",
    "\n",
    "################################################################################################\n",
    "# Add travel time score to states\n",
    "################################################################################################\n",
    "\n",
    "# Travel Time lookup dictionary\n",
    "travel_time = {\n",
    "    (1, 2): 20, (1, 6): 80, (1, 10): 100,\n",
    "    (2, 1): 20, (2, 6): 60, (2, 7): 60, (2, 3): 40, (2, 11): 100,\n",
    "    (3, 2): 40, (3, 7): 60, (3, 8): 60, (3, 4): 20,\n",
    "    (4, 3): 20, (4, 8): 60, (4, 9): 60, (4, 5): 20,\n",
    "    (5, 4): 20, (5, 9): 60, (5, 14): 100,\n",
    "    (6, 1): 80, (6, 2): 60, (6, 10): 60, (6, 11): 40, (6, 7): 20,\n",
    "    (7, 2): 60, (7, 3): 60, (7, 8): 20, (7, 12): 60, (7, 11): 60, (7, 6): 20,\n",
    "    (8, 3): 60, (8, 4): 60, (8, 7): 20, (8, 9): 20, (8, 12): 60, (8, 13): 40,\n",
    "    (9, 4): 60, (9, 5): 60, (9, 8): 20, (9, 13): 60, (9, 14): 60,\n",
    "    (10, 1): 100, (10, 6): 60, (10, 11): 20,\n",
    "    (11, 10): 20, (11, 6): 40, (11, 2): 100, (11, 7): 60, (11, 12): 20,\n",
    "    (12, 11): 20, (12, 7): 60, (12, 8): 60, (12, 13): 20,\n",
    "    (13, 12): 20, (13, 8): 40, (13, 9): 60, (13, 14): 20,\n",
    "    (14, 13): 20, (14, 9): 60, (14, 5): 100\n",
    "}\n",
    "\n",
    "# Assign travel time scores for each action\n",
    "travel_time_scores = {\n",
    "    state: {action: travel_time.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for travel time score\n",
    "# define the state and action to retrieve the travel time score\n",
    "state, action = \"S3\", \"S8\"\n",
    "travel_time_score = travel_time_scores[state][action]\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Function to dynamically calculate the reward based on the state and action, using the scores\n",
    "################################################################################################\n",
    "\n",
    "# Define the normalization ranges (assuming max and min possible values for each score type)\n",
    "WEATHER_MAX = 1000\n",
    "SAFETY_MAX = 1000\n",
    "TRAVEL_TIME_MAX = 1000\n",
    "\n",
    "# Define the weights for each score (adjust based on their importance)\n",
    "WEATHER_WEIGHT = 0.3\n",
    "SAFETY_WEIGHT = 0.4\n",
    "TRAVEL_TIME_WEIGHT = 0.3\n",
    "\n",
    "\n",
    "def normalize_score(score, max_score):\n",
    "    \"\"\" Normalize the score based on the maximum score for each category. \"\"\"\n",
    "    return score / max_score\n",
    "\n",
    "\n",
    "def calculate_reward(state, action):\n",
    "    \"\"\" \n",
    "    Smarter reward function that considers normalized scores and weighted importance. \n",
    "    Incorporates dynamic scaling for weather, safety, and travel time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve individual scores\n",
    "    weather_score = weather_scores[state][action]\n",
    "    safety_score = safety_scores[state][action]\n",
    "    travel_time_score = travel_time_scores[state][action]\n",
    "\n",
    "    # Normalize the scores to be between 0 and 1\n",
    "    normalized_weather = normalize_score(weather_score, WEATHER_MAX)\n",
    "    normalized_safety = normalize_score(safety_score, SAFETY_MAX)\n",
    "    normalized_travel_time = normalize_score(\n",
    "        travel_time_score, TRAVEL_TIME_MAX)\n",
    "\n",
    "    # Calculate weighted sum of normalized scores\n",
    "    total_score = (WEATHER_WEIGHT * normalized_weather +\n",
    "                   SAFETY_WEIGHT * normalized_safety +\n",
    "                   TRAVEL_TIME_WEIGHT * normalized_travel_time)\n",
    "\n",
    "    # Reward is the negative of the total score (since higher scores should be worse)\n",
    "    reward = -1 * total_score\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Example usage: calculate reward for state \"S3\" and action \"S8\"\n",
    "state, action = \"S3\", \"S8\"\n",
    "reward = calculate_reward(state, action)\n",
    "print(f\"Reward for state {state} and action {action}: {reward}\")\n",
    "\n",
    "################################################################################################\n",
    "# Fill up the Rewards dictionary\n",
    "################################################################################################\n",
    "\n",
    "# Fill up the Rewards dictionary to make sure it's a dictionary of dictionaries\n",
    "rewards = {\n",
    "    state: {action: calculate_reward(state, action) for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "\n",
    "# # Print the rewards table in a readable format\n",
    "# print(\"Rewards Table:\")\n",
    "# print(\n",
    "#     \"State -> [Action1, Action2, ..., ActionN] -> [Reward1, Reward2, ..., RewardN]\")\n",
    "\n",
    "# for state, neighbors in actions.items():\n",
    "#     rewards_for_state = rewards[state]\n",
    "#     print(f\"{state} -> {neighbors} -> {rewards_for_state}\")\n",
    "\n",
    "################################################################################################\n",
    "# Implement transition probabilities\n",
    "################################################################################################\n",
    "\n",
    "# Define the transition probabilities for each state-action pair\n",
    "def generate_transition_probability(state, action, states):\n",
    "    # Assuming a random probability for demonstration (this can be customized)\n",
    "    prob_sum = 0\n",
    "    transition_probabilities = {}\n",
    "\n",
    "    # Assigning random probabilities to all possible transitions\n",
    "    for next_state in states:\n",
    "        if next_state != state:\n",
    "            # Generate a random probability between 0 and 1\n",
    "            prob = random.uniform(0, 1)\n",
    "            transition_probabilities[next_state] = prob\n",
    "            prob_sum += prob\n",
    "\n",
    "    # Normalize probabilities to ensure they sum up to 1\n",
    "    for next_state in transition_probabilities:\n",
    "        transition_probabilities[next_state] /= prob_sum\n",
    "\n",
    "    return transition_probabilities\n",
    "\n",
    "\n",
    "# Create transition probabilities for each state-action pair\n",
    "transition_probabilities = {\n",
    "    state: {\n",
    "        action: generate_transition_probability(state, action, actions.keys())\n",
    "        for action in neighbors\n",
    "    }\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "        \n",
    "################################################################################################\n",
    "# Implement transition probabilities\n",
    "################################################################################################\n",
    "\n",
    "# Initialize and solve the MDP\n",
    "route_planner = RoutePlannerMDP(\n",
    "    states, actions, transition_probabilities, rewards)\n",
    "route_planner.value_iteration()\n",
    "\n",
    "# Find optimal route from S1\n",
    "optimal_route = route_planner.get_optimal_route(\"S1\")\n",
    "print(\"Optimal Route:\", optimal_route)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
