{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Route: ['S1', np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), np.str_('S6'), 'Max Steps Exceeded']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RoutePlannerMDP:\n",
    "    def __init__(self, states, actions, transition_probabilities, rewards, gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.P = transition_probabilities  # Transition probability matrix P(s' | s, a)\n",
    "        self.R = rewards  # Reward matrix R(s, a)\n",
    "        self.gamma = gamma  # Discount factor for future rewards\n",
    "        self.policy = {s: np.random.choice(actions) for s in states}  # Initialize random policy\n",
    "\n",
    "    def value_iteration(self, theta=1e-6):\n",
    "        \"\"\"\n",
    "        Solve MDP using Value Iteration algorithm.\n",
    "        \"\"\"\n",
    "        V = {s: 0 for s in self.states}  # Initialize value function\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = V[s]\n",
    "                max_value = float('-inf')\n",
    "                for a in self.actions:\n",
    "                    action_value = sum(self.P[s].get(a, {}).get(s_next, 0) * (self.R[s].get(a, 0) + self.gamma * V.get(s_next, 0))\n",
    "                                       for s_next in self.states)\n",
    "                    max_value = max(max_value, action_value)\n",
    "                V[s] = max_value\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        # Update policy based on value function\n",
    "        for s in self.states:\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for a in self.actions:\n",
    "                action_value = sum(self.P[s].get(a, {}).get(s_next, 0) * (self.R[s].get(a, 0) + self.gamma * V.get(s_next, 0))\n",
    "                                   for s_next in self.states)\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = a\n",
    "            self.policy[s] = best_action\n",
    "\n",
    "    def get_optimal_route(self, start_state, max_steps=100):\n",
    "        \"\"\"\n",
    "        Get the best route from the start state based on the computed policy.\n",
    "        Add a max_steps limit to avoid infinite loops.\n",
    "        \"\"\"\n",
    "        route = [start_state]\n",
    "        current_state = start_state\n",
    "        steps = 0\n",
    "        # Stop if destination reached or step limit exceeded\n",
    "        while current_state != \"S10\" and steps < max_steps:\n",
    "            action = self.policy[current_state]\n",
    "            next_states = list(self.P[current_state][action].keys())\n",
    "            probabilities = list(self.P[current_state][action].values())\n",
    "            current_state = np.random.choice(next_states, p=probabilities)\n",
    "            route.append(current_state)\n",
    "            steps += 1\n",
    "        if current_state != \"S10\":\n",
    "            # Indicate if route doesn't reach destination within max_steps\n",
    "            route.append(\"Max Steps Exceeded\")\n",
    "        return route\n",
    "\n",
    "\n",
    "\n",
    "# Define the states\n",
    "states = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\"]\n",
    "\n",
    "# Define possible actions\n",
    "actions = [\"Continue\", \"Re-route\", \"Adjust Speed\", \"Choose Alternate\"]\n",
    "\n",
    "\n",
    "class RoutePlannerMDP:\n",
    "    def __init__(self, states, actions, transition_probabilities, rewards, gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.P = transition_probabilities\n",
    "        self.R = rewards\n",
    "        self.gamma = gamma\n",
    "        self.policy = {s: np.random.choice(actions) for s in states}\n",
    "\n",
    "    def value_iteration(self, theta=1e-6):\n",
    "        V = {s: 0 for s in self.states}\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = V[s]\n",
    "                max_value = float('-inf')\n",
    "                for a in self.actions:\n",
    "                    action_value = sum(self.P[s].get(a, {}).get(s_next, 0) * (self.R[s].get(a, 0) + self.gamma * V.get(s_next, 0))\n",
    "                                       for s_next in self.states)\n",
    "                    max_value = max(max_value, action_value)\n",
    "                V[s] = max_value\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        for s in self.states:\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for a in self.actions:\n",
    "                action_value = sum(self.P[s].get(a, {}).get(s_next, 0) * (self.R[s].get(a, 0) + self.gamma * V.get(s_next, 0))\n",
    "                                   for s_next in self.states)\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = a\n",
    "            self.policy[s] = best_action\n",
    "\n",
    "    def get_optimal_route(self, start_state, max_steps=100):\n",
    "        route = [start_state]\n",
    "        current_state = start_state\n",
    "        steps = 0\n",
    "        while current_state != \"S10\" and steps < max_steps:\n",
    "            action = self.policy[current_state]\n",
    "            next_states = list(self.P[current_state][action].keys())\n",
    "            probabilities = list(self.P[current_state][action].values())\n",
    "            current_state = np.random.choice(next_states, p=probabilities)\n",
    "            route.append(current_state)\n",
    "            steps += 1\n",
    "        if current_state != \"S10\":\n",
    "            route.append(\"Max Steps Exceeded\")\n",
    "        return route\n",
    "\n",
    "\n",
    "states = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\"]\n",
    "actions = [\"Continue\", \"Re-route\", \"Adjust Speed\", \"Choose Alternate\"]\n",
    "\n",
    "transition_probabilities = {\n",
    "    \"S1\": {\n",
    "        \"Continue\": {\"S2\": 0.7, \"S3\": 0.3},\n",
    "        \"Re-route\": {\"S6\": 1.0},\n",
    "        \"Adjust Speed\": {\"S1\": 1.0},\n",
    "        \"Choose Alternate\": {\"S4\": 0.6, \"S5\": 0.4}\n",
    "    },\n",
    "    \"S2\": {\n",
    "        \"Continue\": {\"S3\": 0.8, \"S4\": 0.2},\n",
    "        \"Re-route\": {\"S6\": 0.7, \"S5\": 0.3},\n",
    "        \"Adjust Speed\": {\"S2\": 1.0},\n",
    "        \"Choose Alternate\": {\"S7\": 0.5, \"S4\": 0.5}\n",
    "    },\n",
    "    \"S3\": {\n",
    "        \"Continue\": {\"S5\": 0.6, \"S7\": 0.4},\n",
    "        \"Re-route\": {\"S8\": 0.6, \"S6\": 0.4},\n",
    "        \"Adjust Speed\": {\"S3\": 1.0},\n",
    "        \"Choose Alternate\": {\"S4\": 0.5, \"S9\": 0.5}\n",
    "    },\n",
    "    \"S4\": {\n",
    "        \"Continue\": {\"S5\": 0.7, \"S6\": 0.3},\n",
    "        \"Re-route\": {\"S8\": 0.5, \"S9\": 0.5},\n",
    "        \"Adjust Speed\": {\"S4\": 1.0},\n",
    "        \"Choose Alternate\": {\"S7\": 0.5, \"S10\": 0.5}\n",
    "    },\n",
    "    \"S5\": {\n",
    "        \"Continue\": {\"S6\": 0.8, \"S7\": 0.2},\n",
    "        \"Re-route\": {\"S9\": 0.7, \"S10\": 0.3},\n",
    "        \"Adjust Speed\": {\"S5\": 1.0},\n",
    "        \"Choose Alternate\": {\"S4\": 0.5, \"S8\": 0.5}\n",
    "    },\n",
    "    \"S6\": {\n",
    "        \"Continue\": {\"S7\": 0.9, \"S8\": 0.1},\n",
    "        \"Re-route\": {\"S6\": 1.0},\n",
    "        \"Adjust Speed\": {\"S6\": 1.0},\n",
    "        \"Choose Alternate\": {\"S4\": 0.3, \"S10\": 0.7}\n",
    "    },\n",
    "    \"S7\": {\n",
    "        \"Continue\": {\"S10\": 0.95, \"S6\": 0.05},\n",
    "        \"Re-route\": {\"S4\": 0.7, \"S9\": 0.3},\n",
    "        \"Adjust Speed\": {\"S7\": 1.0},\n",
    "        \"Choose Alternate\": {\"S6\": 0.3, \"S3\": 0.7}\n",
    "    },\n",
    "    \"S8\": {\n",
    "        \"Continue\": {\"S9\": 0.8, \"S6\": 0.2},\n",
    "        \"Re-route\": {\"S7\": 0.6, \"S10\": 0.4},\n",
    "        \"Adjust Speed\": {\"S8\": 1.0},\n",
    "        \"Choose Alternate\": {\"S7\": 0.3, \"S4\": 0.7}\n",
    "    },\n",
    "    \"S9\": {\n",
    "        \"Continue\": {\"S10\": 1.0},\n",
    "        \"Re-route\": {\"S7\": 0.5, \"S4\": 0.5},\n",
    "        \"Adjust Speed\": {\"S9\": 1.0},\n",
    "        \"Choose Alternate\": {\"S3\": 0.6, \"S7\": 0.4}\n",
    "    },\n",
    "    \"S10\": {\n",
    "        \"Continue\": {\"S10\": 0.0},\n",
    "        \"Re-route\": {\"S10\": 0.0},\n",
    "        \"Adjust Speed\": {\"S10\": 0.0},\n",
    "        \"Choose Alternate\": {\"S10\": 0.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    \"S1\": {\"Continue\": 5, \"Re-route\": 2, \"Adjust Speed\": 3, \"Choose Alternate\": 4},\n",
    "    \"S2\": {\"Continue\": 6, \"Re-route\": 3, \"Adjust Speed\": 4, \"Choose Alternate\": 5},\n",
    "    \"S3\": {\"Continue\": 4, \"Re-route\": 5, \"Adjust Speed\": 2, \"Choose Alternate\": 6},\n",
    "    \"S4\": {\"Continue\": 7, \"Re-route\": 3, \"Adjust Speed\": 4, \"Choose Alternate\": 8},\n",
    "    \"S5\": {\"Continue\": 8, \"Re-route\": 2, \"Adjust Speed\": 3, \"Choose Alternate\": 7},\n",
    "    \"S6\": {\"Continue\": 2, \"Re-route\": 10, \"Adjust Speed\": 1, \"Choose Alternate\": 3},\n",
    "    \"S7\": {\"Continue\": 9, \"Re-route\": 4, \"Adjust Speed\": 3, \"Choose Alternate\": 6},\n",
    "    \"S8\": {\"Continue\": -1, \"Re-route\": 5, \"Adjust Speed\": 2, \"Choose Alternate\": -2},\n",
    "    \"S9\": {\"Continue\": -3, \"Re-route\": 6, \"Adjust Speed\": 3, \"Choose Alternate\": 5},\n",
    "    \"S10\": {\"Continue\": 10, \"Re-route\": 0, \"Adjust Speed\": 1, \"Choose Alternate\": 2}\n",
    "}\n",
    "\n",
    "route_planner = RoutePlannerMDP(\n",
    "    states, actions, transition_probabilities, rewards)\n",
    "route_planner.value_iteration()\n",
    "\n",
    "optimal_route = route_planner.get_optimal_route(\"S1\")\n",
    "print(\"Optimal Route:\", optimal_route)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
