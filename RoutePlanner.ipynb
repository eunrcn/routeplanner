{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for state S3 and action S8: -0.037500000000000006\n",
      "Rewards Table:\n",
      "State -> [Action1, Action2, ..., ActionN] -> [Reward1, Reward2, ..., RewardN]\n",
      "S1 -> ['S2', 'S6', 'S10'] -> [-0.0433, -0.0295, -0.0363]\n",
      "S2 -> ['S1', 'S6', 'S7', 'S3', 'S11'] -> [-0.0433, -0.0271, -0.059399999999999994, -0.0416, -0.048]\n",
      "S3 -> ['S2', 'S7', 'S8', 'S4'] -> [-0.0416, -0.0495, -0.037500000000000006, -0.0375]\n",
      "S4 -> ['S3', 'S8', 'S9', 'S5'] -> [-0.0375, -0.051000000000000004, -0.057800000000000004, -0.0519]\n",
      "S5 -> ['S4', 'S9', 'S14'] -> [-0.0519, -0.060300000000000006, -0.0852]\n",
      "S6 -> ['S1', 'S2', 'S10', 'S11', 'S7'] -> [-0.0295, -0.0271, -0.026099999999999998, -0.034199999999999994, -0.039]\n",
      "S7 -> ['S2', 'S3', 'S8', 'S12', 'S11', 'S6'] -> [-0.059399999999999994, -0.0495, -0.0365, -0.0435, -0.033299999999999996, -0.039]\n",
      "S8 -> ['S3', 'S4', 'S7', 'S9', 'S12', 'S13'] -> [-0.037500000000000006, -0.051000000000000004, -0.0365, -0.056799999999999996, -0.052000000000000005, -0.046200000000000005]\n",
      "S9 -> ['S4', 'S5', 'S8', 'S13', 'S14'] -> [-0.057800000000000004, -0.060300000000000006, -0.056799999999999996, -0.0505, -0.0693]\n",
      "S10 -> ['S1', 'S6', 'S11'] -> [-0.0363, -0.026099999999999998, -0.022]\n",
      "S11 -> ['S10', 'S6', 'S2', 'S7', 'S12'] -> [-0.022, -0.034199999999999994, -0.048, -0.033299999999999996, -0.0298]\n",
      "S12 -> ['S11', 'S7', 'S8', 'S13'] -> [-0.0298, -0.0435, -0.052000000000000005, -0.047400000000000005]\n",
      "S13 -> ['S12', 'S8', 'S9', 'S14'] -> [-0.047400000000000005, -0.046200000000000005, -0.0505, -0.06960000000000001]\n",
      "S14 -> ['S13', 'S9', 'S5'] -> [-0.06960000000000001, -0.0693, -0.0852]\n",
      "Transition Probabilities Table:\n",
      "State S1, Action S2 -> Transition Probabilities: {'S2': 0.14881675739855066, 'S3': 0.06669677838983089, 'S4': 0.0530815364219654, 'S5': 0.011966039186196523, 'S6': 0.09750523503258551, 'S7': 0.0906002382100431, 'S8': 0.13915183913019144, 'S9': 0.010324122181034406, 'S10': 0.002190940047816071, 'S11': 0.0400569943771898, 'S12': 0.14152331482207223, 'S13': 0.04605804820456189, 'S14': 0.15202815659796207}\n",
      "State S1, Action S6 -> Transition Probabilities: {'S2': 0.017018343560688636, 'S3': 0.12467055826674543, 'S4': 0.09822758748018456, 'S5': 0.0570643310846402, 'S6': 0.06345621064316692, 'S7': 0.02689281953219752, 'S8': 0.09222733154593392, 'S9': 0.06812630836883268, 'S10': 0.1420928123528884, 'S11': 0.042736041217069386, 'S12': 0.1282734314438484, 'S13': 0.13222694263405596, 'S14': 0.006987281869748103}\n",
      "State S1, Action S10 -> Transition Probabilities: {'S2': 0.16444011788600588, 'S3': 0.05285600314888389, 'S4': 0.0037915405849997114, 'S5': 0.0012222046275516126, 'S6': 0.1598453602750104, 'S7': 0.11654483065730085, 'S8': 0.08904438955701151, 'S9': 0.18329684718223432, 'S10': 0.06748730426023548, 'S11': 0.04236708534447486, 'S12': 0.025203008230263235, 'S13': 0.026109198921071605, 'S14': 0.06779210932495675}\n",
      "State S2, Action S1 -> Transition Probabilities: {'S1': 0.11603502109907299, 'S3': 0.054110975077506826, 'S4': 0.04613638548302287, 'S5': 0.046606271854317885, 'S6': 0.06416159397427768, 'S7': 0.015927629454325, 'S8': 0.02904759976427326, 'S9': 0.1417741631657152, 'S10': 0.08424848719924574, 'S11': 0.16216602138378794, 'S12': 0.11363377370653199, 'S13': 0.0632615520674707, 'S14': 0.06289052577045193}\n",
      "State S2, Action S6 -> Transition Probabilities: {'S1': 0.04960767699582579, 'S3': 0.08068717072183532, 'S4': 0.09084327754409892, 'S5': 0.09573266436352781, 'S6': 0.03415954991713794, 'S7': 0.03601385692665156, 'S8': 0.07624869774754321, 'S9': 0.10394727528284052, 'S10': 0.07247242921609545, 'S11': 0.11506547191819112, 'S12': 0.038015122607346005, 'S13': 0.12075130047361243, 'S14': 0.08645550628529401}\n",
      "State S2, Action S7 -> Transition Probabilities: {'S1': 0.11085721711823975, 'S3': 0.11662284847337515, 'S4': 0.10519013344413322, 'S5': 0.030109428114739142, 'S6': 0.10250097208019457, 'S7': 0.03829340708575436, 'S8': 0.014336048488463852, 'S9': 0.07836303382049524, 'S10': 0.07958227681162677, 'S11': 0.030542869449702546, 'S12': 0.10131143579435889, 'S13': 0.09554813666393121, 'S14': 0.09674219265498528}\n",
      "State S2, Action S3 -> Transition Probabilities: {'S1': 0.03154245054316242, 'S3': 0.03673474705154507, 'S4': 0.14416156825954468, 'S5': 0.1301929018608463, 'S6': 0.1099895156953746, 'S7': 0.07194627806413294, 'S8': 0.16026720388824492, 'S9': 0.04049783535078655, 'S10': 0.12353833412441313, 'S11': 0.04024754849240158, 'S12': 0.020575895818221345, 'S13': 0.04821086448166527, 'S14': 0.04209485636966112}\n",
      "State S2, Action S11 -> Transition Probabilities: {'S1': 0.11511077184184523, 'S3': 0.019871968216220122, 'S4': 0.00902567323336002, 'S5': 0.08177414053611512, 'S6': 0.03063566499765252, 'S7': 0.047657737959878675, 'S8': 0.0875607423872086, 'S9': 0.012621849854540226, 'S10': 0.04902738193737499, 'S11': 0.12881257878236585, 'S12': 0.15913142386913787, 'S13': 0.12781262646403332, 'S14': 0.13095743992026745}\n",
      "State S3, Action S2 -> Transition Probabilities: {'S1': 0.09336479666662945, 'S2': 0.08388520909433624, 'S4': 0.11946694408427329, 'S5': 0.12238816141602821, 'S6': 0.03262058682910427, 'S7': 0.0997510899557626, 'S8': 0.07480062083144944, 'S9': 0.10346554902340004, 'S10': 0.034594166804255935, 'S11': 0.017671903016671146, 'S12': 0.12764720083164435, 'S13': 0.04519587378503853, 'S14': 0.045147897661406484}\n",
      "State S3, Action S7 -> Transition Probabilities: {'S1': 0.1408298378740124, 'S2': 0.042231389318937156, 'S4': 0.11200380195587113, 'S5': 0.0050924722161067686, 'S6': 0.008927955250344535, 'S7': 0.08021400494225778, 'S8': 0.046656390297148936, 'S9': 0.13059025810731772, 'S10': 0.10793976883842007, 'S11': 0.1265221966973315, 'S12': 0.09614146564711915, 'S13': 0.06728816111924098, 'S14': 0.03556229773589182}\n",
      "State S3, Action S8 -> Transition Probabilities: {'S1': 0.1431064771606337, 'S2': 0.09062122602182539, 'S4': 0.011408456649600534, 'S5': 0.10812156653392532, 'S6': 0.10244857506086145, 'S7': 0.12531226310091098, 'S8': 0.011714292147073249, 'S9': 0.01476385104010151, 'S10': 0.026510344496648713, 'S11': 0.08890672311614675, 'S12': 0.13270434481707724, 'S13': 0.14225153006396402, 'S14': 0.0021303497912308943}\n",
      "State S3, Action S4 -> Transition Probabilities: {'S1': 0.01612782889942356, 'S2': 0.03014120272288431, 'S4': 0.04124143981098241, 'S5': 0.10774770789398737, 'S6': 0.04034695734283109, 'S7': 0.028915760379372376, 'S8': 0.16953435053625862, 'S9': 0.15063759466675344, 'S10': 0.011066054237173406, 'S11': 0.011014298099604964, 'S12': 0.17409988804870427, 'S13': 0.1792992699920687, 'S14': 0.0398276473699556}\n",
      "State S4, Action S3 -> Transition Probabilities: {'S1': 0.001639247163526218, 'S2': 0.11221742029059882, 'S3': 0.05235850121562365, 'S5': 0.05003903191885451, 'S6': 0.1363512439375767, 'S7': 0.08923561087454458, 'S8': 0.09368170325011897, 'S9': 0.10717397326349064, 'S10': 0.10528148720687276, 'S11': 0.04159223502066407, 'S12': 0.10670260153123012, 'S13': 0.10281220505858099, 'S14': 0.0009147392683179085}\n",
      "State S4, Action S8 -> Transition Probabilities: {'S1': 0.10799156914127289, 'S2': 0.09632182492497855, 'S3': 0.009750616870185755, 'S5': 0.006477078941174533, 'S6': 0.041327496662606024, 'S7': 0.125739702379716, 'S8': 0.001245997951276786, 'S9': 0.024855637120393397, 'S10': 0.1263738935983758, 'S11': 0.15327337388213663, 'S12': 0.09955012449724635, 'S13': 0.14283373541030914, 'S14': 0.06425894862032809}\n",
      "State S4, Action S9 -> Transition Probabilities: {'S1': 0.007046710285466597, 'S2': 0.0468132439664158, 'S3': 0.1127686791473982, 'S5': 0.09456017922421579, 'S6': 0.02872832899952125, 'S7': 0.09049856366534806, 'S8': 0.1488670980341076, 'S9': 0.1634762954863193, 'S10': 0.06243723267433794, 'S11': 0.019090973003294088, 'S12': 0.16829217568740773, 'S13': 0.05417174183240507, 'S14': 0.003248777993762449}\n",
      "State S4, Action S5 -> Transition Probabilities: {'S1': 0.07663533297621565, 'S2': 0.10724343961957947, 'S3': 0.05211530813691369, 'S5': 0.09349357269282631, 'S6': 0.12020274212535152, 'S7': 0.08337106105739842, 'S8': 0.10938067820568989, 'S9': 0.09137821615260117, 'S10': 0.05620099933436137, 'S11': 0.025628044390839877, 'S12': 0.003956049642512375, 'S13': 0.12320294326342433, 'S14': 0.05719161240228598}\n",
      "State S5, Action S4 -> Transition Probabilities: {'S1': 0.03524585756466389, 'S2': 0.05126563113149388, 'S3': 0.12384829726525501, 'S4': 0.13242314583307635, 'S6': 0.034757524214707326, 'S7': 0.10663535670747981, 'S8': 0.058316740651331786, 'S9': 0.11431752216599717, 'S10': 0.09531275785376428, 'S11': 0.028654141815282242, 'S12': 0.11074237590396373, 'S13': 0.053510196209600666, 'S14': 0.054970452683383866}\n",
      "State S5, Action S9 -> Transition Probabilities: {'S1': 0.026271265976462947, 'S2': 0.08506359023603925, 'S3': 0.0834408462582753, 'S4': 0.11123262530261802, 'S6': 0.10488199357379714, 'S7': 0.09416992278921911, 'S8': 0.09114444080429504, 'S9': 0.03604168446357936, 'S10': 0.12526354239604223, 'S11': 0.0733921800437787, 'S12': 0.130908820259789, 'S13': 0.011342195550826873, 'S14': 0.026846892345277047}\n",
      "State S5, Action S14 -> Transition Probabilities: {'S1': 0.12375523608765976, 'S2': 0.032748436601885485, 'S3': 0.06864911197469063, 'S4': 0.05560503395220579, 'S6': 0.13265053413347105, 'S7': 0.016608251909514364, 'S8': 0.06636875878559068, 'S9': 0.03502668840983941, 'S10': 0.12628849283578258, 'S11': 0.0991052582794703, 'S12': 0.1249671129498312, 'S13': 0.07438538982947847, 'S14': 0.043841694250580314}\n",
      "State S6, Action S1 -> Transition Probabilities: {'S1': 0.10672014348383785, 'S2': 0.09736500131135471, 'S3': 0.04850952199223139, 'S4': 0.06950496838778249, 'S5': 0.07023200076441126, 'S7': 0.09708613791918341, 'S8': 0.08489471776118983, 'S9': 0.03730755461866163, 'S10': 0.09886551853061823, 'S11': 0.07923610445577497, 'S12': 0.04544686450594534, 'S13': 0.07386665565651257, 'S14': 0.09096481061249632}\n",
      "State S6, Action S2 -> Transition Probabilities: {'S1': 0.0012341002041900543, 'S2': 0.05145154230751904, 'S3': 0.09993224761935772, 'S4': 0.010799662545505523, 'S5': 0.09214667775938971, 'S7': 0.12433131419043367, 'S8': 0.11726394786499325, 'S9': 0.03418705722980003, 'S10': 0.1038386693629134, 'S11': 0.0936469206623897, 'S12': 0.0700732952468452, 'S13': 0.1071628453789088, 'S14': 0.09393171962775394}\n",
      "State S6, Action S10 -> Transition Probabilities: {'S1': 0.08301899302760933, 'S2': 0.12879087665708372, 'S3': 0.03862069188905445, 'S4': 0.14573988092193915, 'S5': 0.07510409353857944, 'S7': 0.013794855490335862, 'S8': 0.08686649481490953, 'S9': 0.0731556065446359, 'S10': 0.1283528717633827, 'S11': 0.09334559168395151, 'S12': 0.050672123273706855, 'S13': 0.014899542724649745, 'S14': 0.06763837767016187}\n",
      "State S6, Action S11 -> Transition Probabilities: {'S1': 0.14569486256583314, 'S2': 0.09659860147751194, 'S3': 0.04201739396611055, 'S4': 0.003796524851732938, 'S5': 0.023697369858299917, 'S7': 0.1231836106296965, 'S8': 0.023132789648030384, 'S9': 0.15390123717723583, 'S10': 0.06119820299150916, 'S11': 0.07219235070226777, 'S12': 0.027475905699794287, 'S13': 0.16213505944788376, 'S14': 0.06497609098409383}\n",
      "State S6, Action S7 -> Transition Probabilities: {'S1': 0.12363965413475161, 'S2': 0.10240015282750685, 'S3': 0.06863650559603414, 'S4': 0.09875253655274234, 'S5': 0.12979790105587016, 'S7': 0.05847860988355577, 'S8': 0.10100863805970316, 'S9': 0.01336437520484547, 'S10': 0.14149434218614373, 'S11': 0.04734004263961501, 'S12': 0.028787535969135046, 'S13': 0.042550502852184306, 'S14': 0.04374920303791245}\n",
      "State S7, Action S2 -> Transition Probabilities: {'S1': 0.039076865106228494, 'S2': 0.08639313385899092, 'S3': 0.05425198352327216, 'S4': 0.10292373618230849, 'S5': 0.027978271077406122, 'S6': 0.060634619890544285, 'S8': 0.08776205528822464, 'S9': 0.11864675389587478, 'S10': 0.12796009174423664, 'S11': 0.14513059591037616, 'S12': 0.1269577584782791, 'S13': 0.011489142328698422, 'S14': 0.01079499271555981}\n",
      "State S7, Action S3 -> Transition Probabilities: {'S1': 0.037792178854668936, 'S2': 0.11311628086216603, 'S3': 0.1642032717588252, 'S4': 0.06805867458207121, 'S5': 0.08517685132720067, 'S6': 0.12389037989430345, 'S8': 0.016662024566572592, 'S9': 0.02237925954970227, 'S10': 0.0910133767924082, 'S11': 0.05970982986181128, 'S12': 0.06690711749217307, 'S13': 0.10814775211010333, 'S14': 0.04294300234799362}\n",
      "State S7, Action S8 -> Transition Probabilities: {'S1': 0.026569839433865133, 'S2': 0.07089423811352182, 'S3': 0.06652560137931632, 'S4': 0.1621076064839648, 'S5': 0.06104902745030523, 'S6': 0.10344009297340719, 'S8': 0.10459423590326423, 'S9': 0.03567427236388688, 'S10': 0.02868505676456166, 'S11': 0.07666320894097332, 'S12': 0.006844014659546227, 'S13': 0.16393358374484088, 'S14': 0.09301922178854635}\n",
      "State S7, Action S12 -> Transition Probabilities: {'S1': 0.06738727632271822, 'S2': 0.11956678144963181, 'S3': 0.024006383068855084, 'S4': 0.10571926675517036, 'S5': 0.03277617010581975, 'S6': 0.10458151213099018, 'S8': 0.025245181754356427, 'S9': 0.11442527978705014, 'S10': 0.1249617756344534, 'S11': 0.04364058639906896, 'S12': 0.13327228523672438, 'S13': 0.07386215686630072, 'S14': 0.030555344488860565}\n",
      "State S7, Action S11 -> Transition Probabilities: {'S1': 0.09888739090533584, 'S2': 0.05354142863875848, 'S3': 0.1369243664113928, 'S4': 0.036607496700404564, 'S5': 0.10480766045833649, 'S6': 0.1020070684636136, 'S8': 0.09264582047402478, 'S9': 0.000916596972637604, 'S10': 0.007140062232624153, 'S11': 0.13828949191983636, 'S12': 0.11106300557861434, 'S13': 0.08231530576938056, 'S14': 0.03485430547504041}\n",
      "State S7, Action S6 -> Transition Probabilities: {'S1': 0.08565908424958991, 'S2': 0.06001037485416361, 'S3': 0.07706664794450847, 'S4': 0.09090900908808146, 'S5': 0.05580103212788144, 'S6': 0.10406301808470536, 'S8': 0.0883769579674244, 'S9': 0.06366432710864924, 'S10': 0.09205011658697031, 'S11': 0.06314236466214276, 'S12': 0.1068876556939585, 'S13': 0.05254673210142607, 'S14': 0.05982267953049836}\n",
      "State S8, Action S3 -> Transition Probabilities: {'S1': 0.030535776774500784, 'S2': 0.06738542458550252, 'S3': 0.17177548832599907, 'S4': 0.04638978137622589, 'S5': 0.08445275383718619, 'S6': 0.044378489747493324, 'S7': 0.005392441008956768, 'S9': 0.09631852106500913, 'S10': 0.04876506922952906, 'S11': 0.15392244757527215, 'S12': 0.06445997872077881, 'S13': 0.10315974191489355, 'S14': 0.08306408583865256}\n",
      "State S8, Action S4 -> Transition Probabilities: {'S1': 0.03592190356338175, 'S2': 0.06790632128100721, 'S3': 0.0751323097869861, 'S4': 0.12672116706355155, 'S5': 0.057321534989443666, 'S6': 0.046268609259065695, 'S7': 0.12844936686941186, 'S9': 0.01782220167505499, 'S10': 0.12694989407656956, 'S11': 0.05970422638473642, 'S12': 0.12807288878821352, 'S13': 0.07063952933629383, 'S14': 0.05909004692628381}\n",
      "State S8, Action S7 -> Transition Probabilities: {'S1': 0.0421094092106872, 'S2': 0.10165800696847829, 'S3': 0.11235292314089895, 'S4': 0.16290561314390364, 'S5': 0.03962658526804743, 'S6': 0.02375209249116957, 'S7': 0.02077512358623587, 'S9': 0.037133876351887966, 'S10': 0.1339614104099226, 'S11': 0.15137572579724992, 'S12': 0.08109015092067907, 'S13': 0.027820097888972422, 'S14': 0.06543898482186697}\n",
      "State S8, Action S9 -> Transition Probabilities: {'S1': 0.058140947483572986, 'S2': 0.07135584069432709, 'S3': 0.12591600941658387, 'S4': 0.013615017301429744, 'S5': 0.21714251287881095, 'S6': 0.17509750293828247, 'S7': 0.03386417022532586, 'S9': 0.05973860162941404, 'S10': 0.08479541350163795, 'S11': 0.018253424242255487, 'S12': 0.030211613151386332, 'S13': 0.04477797428939723, 'S14': 0.06709097224757612}\n",
      "State S8, Action S12 -> Transition Probabilities: {'S1': 0.1465486468340245, 'S2': 0.1314646699589297, 'S3': 0.011026805978857326, 'S4': 0.0483133922559983, 'S5': 0.11832452278770839, 'S6': 0.12661725545297337, 'S7': 0.033247076078634516, 'S9': 0.07013866193479505, 'S10': 0.07824182314435686, 'S11': 0.0418915353617038, 'S12': 0.03496754719660111, 'S13': 0.024361910096441057, 'S14': 0.13485615291897618}\n",
      "State S8, Action S13 -> Transition Probabilities: {'S1': 0.0694095432157022, 'S2': 0.019641716236258322, 'S3': 0.08220481757061603, 'S4': 0.024964687476925287, 'S5': 0.08269320128204957, 'S6': 0.1743121385104355, 'S7': 0.15736947304357088, 'S9': 0.03089269540991898, 'S10': 0.1738268669037367, 'S11': 0.07299164190863106, 'S12': 0.08273129827514626, 'S13': 0.007151142212851669, 'S14': 0.0218107779541577}\n",
      "State S9, Action S4 -> Transition Probabilities: {'S1': 0.13286265443065753, 'S2': 0.12157812170928427, 'S3': 0.08985578515041774, 'S4': 0.07606510411389741, 'S5': 0.022428847477201978, 'S6': 0.03139351859371352, 'S7': 0.06488787854874078, 'S8': 0.004135290216120771, 'S10': 0.09505131796356153, 'S11': 0.027406671466020664, 'S12': 0.15004726635148613, 'S13': 0.10000865887190424, 'S14': 0.08427888510699334}\n",
      "State S9, Action S5 -> Transition Probabilities: {'S1': 0.08310644101452015, 'S2': 0.11802720349672752, 'S3': 0.04037469003495332, 'S4': 0.11776708749037447, 'S5': 0.04823001205450622, 'S6': 0.062379262806305354, 'S7': 0.07461786993083443, 'S8': 0.09730427763498449, 'S10': 0.09339557732499829, 'S11': 0.0838578730940981, 'S12': 0.020580098708736328, 'S13': 0.0355271970489109, 'S14': 0.12483240936005033}\n",
      "State S9, Action S8 -> Transition Probabilities: {'S1': 0.07369987705846526, 'S2': 0.09139814949676937, 'S3': 0.013984467784682602, 'S4': 0.07485727645023146, 'S5': 0.11116501724252577, 'S6': 0.0738486038668911, 'S7': 0.05514358371399472, 'S8': 0.06596525054393695, 'S10': 0.08458998204049586, 'S11': 0.116225458373596, 'S12': 0.09309139736943618, 'S13': 0.06318007392335082, 'S14': 0.08285086213562384}\n",
      "State S9, Action S13 -> Transition Probabilities: {'S1': 0.045102286652389606, 'S2': 0.12285877441048401, 'S3': 0.13314187627412383, 'S4': 0.02497710272535911, 'S5': 0.029397222217390086, 'S6': 0.04430380649325375, 'S7': 0.13281495996190187, 'S8': 0.07701434161287904, 'S10': 0.07252326676408355, 'S11': 0.03816257057199083, 'S12': 0.1394219494931094, 'S13': 0.02230768944550578, 'S14': 0.11797415337752927}\n",
      "State S9, Action S14 -> Transition Probabilities: {'S1': 0.08344077179045115, 'S2': 0.05500728778193127, 'S3': 0.04542683841919888, 'S4': 0.13193529205545532, 'S5': 0.11621879543162351, 'S6': 0.010801213770005376, 'S7': 0.10913202501573185, 'S8': 0.12835965489759618, 'S10': 0.032474762507221575, 'S11': 0.018059455695946774, 'S12': 0.010955506313862516, 'S13': 0.12086036419611887, 'S14': 0.13732803212485686}\n",
      "State S10, Action S1 -> Transition Probabilities: {'S1': 0.0467080896932046, 'S2': 0.0955141368308258, 'S3': 0.10706443359667689, 'S4': 0.15690078320305004, 'S5': 0.05499067280077539, 'S6': 0.14949040427472293, 'S7': 0.12262598787113543, 'S8': 0.06977673314743055, 'S9': 0.060146549229923546, 'S11': 0.016489334423114288, 'S12': 0.01714955967530259, 'S13': 0.0883611865083764, 'S14': 0.014782128745461639}\n",
      "State S10, Action S6 -> Transition Probabilities: {'S1': 0.07076448582955179, 'S2': 0.028010585777571043, 'S3': 0.13333166607158908, 'S4': 0.10495905978368875, 'S5': 0.035742839968078824, 'S6': 0.09511018877501803, 'S7': 0.08600940600645131, 'S8': 0.03304691945995818, 'S9': 0.11476076005566008, 'S11': 0.09416759034050476, 'S12': 0.11467057960073701, 'S13': 0.05618188252751673, 'S14': 0.03324403580367436}\n",
      "State S10, Action S11 -> Transition Probabilities: {'S1': 0.07040234187088178, 'S2': 0.06893895798372532, 'S3': 0.08869775966931122, 'S4': 0.023015768210223185, 'S5': 0.08094218450330268, 'S6': 0.09059787499076795, 'S7': 0.04736632371771391, 'S8': 0.08350591457306726, 'S9': 0.10263371414240391, 'S11': 0.08692263011786537, 'S12': 0.028031349179491374, 'S13': 0.1236756758940793, 'S14': 0.1052695051471668}\n",
      "State S11, Action S10 -> Transition Probabilities: {'S1': 0.10468175343767777, 'S2': 0.07722256682812079, 'S3': 0.11203680540924353, 'S4': 0.0946179562198236, 'S5': 0.1290571164995355, 'S6': 0.06261190576138358, 'S7': 0.117124028094564, 'S8': 0.1306370004713164, 'S9': 0.07937512645908439, 'S10': 0.0026137398855413867, 'S12': 0.023786881646551267, 'S13': 0.05376136294429636, 'S14': 0.012473756342861351}\n",
      "State S11, Action S6 -> Transition Probabilities: {'S1': 0.11492418633691542, 'S2': 0.05663969514764323, 'S3': 0.05513499659123461, 'S4': 0.041116795581382704, 'S5': 0.08410207649207559, 'S6': 0.11070897095503696, 'S7': 0.11436469797625368, 'S8': 0.018743497733730206, 'S9': 0.06962472833366538, 'S10': 0.11821648682465262, 'S12': 0.06478463341627577, 'S13': 0.11049801571075975, 'S14': 0.041141218900374185}\n",
      "State S11, Action S2 -> Transition Probabilities: {'S1': 0.11793052909231382, 'S2': 0.04979387653848931, 'S3': 0.13832032123494076, 'S4': 0.07326105164427778, 'S5': 0.08999772877351239, 'S6': 0.02394948745213731, 'S7': 0.00033590300310613, 'S8': 0.006656157826125974, 'S9': 0.11133421652532281, 'S10': 0.11887982346431904, 'S12': 0.13440029791790964, 'S13': 0.060057097722253316, 'S14': 0.07508350880529177}\n",
      "State S11, Action S7 -> Transition Probabilities: {'S1': 0.0540530460263019, 'S2': 0.10359243297339214, 'S3': 0.050650558768698783, 'S4': 0.07223029965581321, 'S5': 0.12260177808221856, 'S6': 0.08223821609270485, 'S7': 0.0441580570457464, 'S8': 0.003936778254350214, 'S9': 0.11691246939204597, 'S10': 0.053858994325457044, 'S12': 0.07456835532619935, 'S13': 0.14463970995951925, 'S14': 0.07655930409755234}\n",
      "State S11, Action S12 -> Transition Probabilities: {'S1': 0.12309919369615112, 'S2': 0.11794228950128229, 'S3': 0.05262902468824267, 'S4': 0.061020287063982236, 'S5': 0.08602626975838494, 'S6': 0.045837119495846346, 'S7': 0.0970638975261543, 'S8': 0.05231030648496332, 'S9': 0.02596299040641757, 'S10': 0.09468999427156821, 'S12': 0.06637363184655198, 'S13': 0.12247354002704289, 'S14': 0.054571455233412156}\n",
      "State S12, Action S11 -> Transition Probabilities: {'S1': 0.005506569170390391, 'S2': 0.03681774206614073, 'S3': 0.074757288264278, 'S4': 0.05465290056403486, 'S5': 0.13189247991928582, 'S6': 0.11786069206170373, 'S7': 0.1338084108676517, 'S8': 0.030956085454182537, 'S9': 0.11332281581836377, 'S10': 0.06507526822661382, 'S11': 0.09002403736753244, 'S13': 0.07325698575405878, 'S14': 0.07206872446576358}\n",
      "State S12, Action S7 -> Transition Probabilities: {'S1': 0.11830569877095189, 'S2': 0.13393080434617008, 'S3': 0.10766040296635272, 'S4': 0.14036636003913558, 'S5': 0.04629618649045661, 'S6': 0.0025756676598518175, 'S7': 0.027679630377831523, 'S8': 0.058064508140985134, 'S9': 0.10105756998363744, 'S10': 0.06679101484051958, 'S11': 0.0792132849807587, 'S13': 0.06829389887499501, 'S14': 0.04976497252835395}\n",
      "State S12, Action S8 -> Transition Probabilities: {'S1': 0.11536049063786899, 'S2': 0.15257118082964274, 'S3': 0.032774293051579015, 'S4': 0.012518837702236087, 'S5': 0.07989359059090342, 'S6': 0.07789752066787634, 'S7': 0.035551908296783175, 'S8': 0.03997437503600513, 'S9': 0.11828402830313335, 'S10': 0.047384236208723736, 'S11': 0.13759638685690562, 'S13': 0.11517874571701313, 'S14': 0.03501440610132923}\n",
      "State S12, Action S13 -> Transition Probabilities: {'S1': 0.05836728041328628, 'S2': 0.10424220442712841, 'S3': 0.11786403313280794, 'S4': 0.06448157742589518, 'S5': 0.037791148465814585, 'S6': 0.016716810366537407, 'S7': 0.13275023930931115, 'S8': 0.12416806343648217, 'S9': 0.028690746138774197, 'S10': 0.01273180691419667, 'S11': 0.08306223641493095, 'S13': 0.08001716419583507, 'S14': 0.13911668935900004}\n",
      "State S13, Action S12 -> Transition Probabilities: {'S1': 0.11136275586184151, 'S2': 0.03678853188506877, 'S3': 0.07943439065901733, 'S4': 0.07688375387325082, 'S5': 0.10869382406873015, 'S6': 0.007348189729156675, 'S7': 0.11317804632124993, 'S8': 0.08721411273813989, 'S9': 0.1095130662703807, 'S10': 0.03855420285750519, 'S11': 0.11250053270335171, 'S12': 0.018108522399177332, 'S14': 0.10042007063313003}\n",
      "State S13, Action S8 -> Transition Probabilities: {'S1': 0.054893907532707835, 'S2': 0.019847073420384814, 'S3': 0.06537587589942782, 'S4': 0.06826456768461321, 'S5': 0.04029252087661384, 'S6': 0.1452519256910268, 'S7': 0.09428673622877404, 'S8': 0.08942121705784607, 'S9': 0.11157005972678659, 'S10': 0.10613620947675219, 'S11': 0.09525076749567946, 'S12': 0.05529372673002307, 'S14': 0.05411541217936418}\n",
      "State S13, Action S9 -> Transition Probabilities: {'S1': 0.03332593792656949, 'S2': 0.05263375999948799, 'S3': 0.11287976222367169, 'S4': 0.0021174156780591454, 'S5': 0.1315403063967903, 'S6': 0.005964203995560855, 'S7': 0.052764056894972926, 'S8': 0.06753459233210161, 'S9': 0.1699225949909666, 'S10': 0.1268445139007192, 'S11': 0.04511144996632889, 'S12': 0.07223210174073401, 'S14': 0.12712930395403732}\n",
      "State S13, Action S14 -> Transition Probabilities: {'S1': 0.05006816614133561, 'S2': 0.002677853923266965, 'S3': 0.12492240345214198, 'S4': 0.046212010663711, 'S5': 0.12772331150037086, 'S6': 0.08517237243008917, 'S7': 0.11720160492906696, 'S8': 0.01839047771862725, 'S9': 0.06779420357713255, 'S10': 0.06783121214800951, 'S11': 0.1114650180570479, 'S12': 0.05007363224749206, 'S14': 0.13046773321170815}\n",
      "State S14, Action S13 -> Transition Probabilities: {'S1': 0.14766374761064305, 'S2': 0.1624203726878716, 'S3': 0.10119965340702802, 'S4': 0.02319251992342459, 'S5': 0.13387597624462844, 'S6': 0.05161451098489193, 'S7': 0.016807785140007702, 'S8': 0.08285890681306275, 'S9': 0.07328587008903148, 'S10': 0.025491388605392226, 'S11': 0.0989770608039377, 'S12': 0.015686261096212434, 'S13': 0.06692594659386816}\n",
      "State S14, Action S9 -> Transition Probabilities: {'S1': 0.14541222253507594, 'S2': 0.003277514130735469, 'S3': 0.02074231266728275, 'S4': 0.12322115690604851, 'S5': 0.06518812294892248, 'S6': 0.12891499976981355, 'S7': 0.06651886368194629, 'S8': 0.19279897261802215, 'S9': 0.04174030189113449, 'S10': 0.03556601177308506, 'S11': 0.028152514763895145, 'S12': 0.05958623004435826, 'S13': 0.08888077626968}\n",
      "State S14, Action S5 -> Transition Probabilities: {'S1': 0.11074956863076832, 'S2': 0.05750603652348775, 'S3': 0.06485155960892863, 'S4': 0.06609958235801892, 'S5': 0.062137334064713265, 'S6': 0.10520449033337047, 'S7': 0.11262217305433209, 'S8': 0.045510002266198835, 'S9': 0.056460111787337605, 'S10': 0.0643949511972453, 'S11': 0.12120498702180345, 'S12': 0.049408522486596954, 'S13': 0.08385068066719843}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 303\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Initialize and solve the MDP\u001b[39;00m\n\u001b[1;32m    301\u001b[0m route_planner \u001b[38;5;241m=\u001b[39m RoutePlannerMDP(\n\u001b[1;32m    302\u001b[0m     states, actions, transition_probabilities, rewards)\n\u001b[0;32m--> 303\u001b[0m \u001b[43mroute_planner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Find optimal route from S1\u001b[39;00m\n\u001b[1;32m    306\u001b[0m optimal_route \u001b[38;5;241m=\u001b[39m route_planner\u001b[38;5;241m.\u001b[39mget_optimal_route(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mRoutePlannerMDP.value_iteration\u001b[0;34m(self, theta)\u001b[0m\n\u001b[1;32m     20\u001b[0m max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[s]:\n\u001b[0;32m---> 22\u001b[0m     action_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms_next\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms_next\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms_next\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_value, action_value)\n\u001b[1;32m     26\u001b[0m V[s] \u001b[38;5;241m=\u001b[39m max_value\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[s]:\n\u001b[1;32m     22\u001b[0m     action_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[s][a][s_next] \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 23\u001b[0m                        (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m V[s_next])\n\u001b[1;32m     24\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m s_next \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[s][a])\n\u001b[1;32m     25\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_value, action_value)\n\u001b[1;32m     26\u001b[0m V[s] \u001b[38;5;241m=\u001b[39m max_value\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RoutePlannerMDP:\n",
    "    def __init__(self, states, actions, transition_probabilities, rewards, gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.P = transition_probabilities\n",
    "        self.R = rewards\n",
    "        self.gamma = gamma\n",
    "        self.policy = {s: np.random.choice(actions[s]) for s in states}\n",
    "\n",
    "    def value_iteration(self, theta=1e-6):\n",
    "        V = {s: 0 for s in self.states}\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = V[s]\n",
    "                max_value = float('-inf')\n",
    "                for a in self.actions[s]:\n",
    "                    action_value = sum(self.P[s][a][s_next] *\n",
    "                                       (self.R[s][a] + self.gamma * V[s_next])\n",
    "                                       for s_next in self.P[s][a])\n",
    "                    max_value = max(max_value, action_value)\n",
    "                V[s] = max_value\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        for s in self.states:\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for a in self.actions[s]:\n",
    "                action_value = sum(self.P[s][a][s_next] *\n",
    "                                   (self.R[s][a] + self.gamma * V[s_next])\n",
    "                                   for s_next in self.P[s][a])\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = a\n",
    "            self.policy[s] = best_action\n",
    "\n",
    "    def get_optimal_route(self, start_state, max_steps=100):\n",
    "        route = [start_state]\n",
    "        current_state = start_state\n",
    "        steps = 0\n",
    "        while current_state != \"S14\" and steps < max_steps:\n",
    "            action = self.policy[current_state]\n",
    "            next_states = list(self.P[current_state][action].keys())\n",
    "            probabilities = list(self.P[current_state][action].values())\n",
    "            current_state = np.random.choice(next_states, p=probabilities)\n",
    "            route.append(current_state)\n",
    "            steps += 1\n",
    "        if current_state != \"S14\":\n",
    "            route.append(\"Max Steps Exceeded\")\n",
    "        return route\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Define states and actions\n",
    "states = [f\"S{i}\" for i in range(1, 15)]\n",
    "\n",
    "actions = {\n",
    "    \"S1\": [\"S2\", \"S6\", \"S10\"],\n",
    "    \"S2\": [\"S1\", \"S6\", \"S7\", \"S3\", \"S11\"],\n",
    "    \"S3\": [\"S2\", \"S7\", \"S8\", \"S4\"],\n",
    "    \"S4\": [\"S3\", \"S8\", \"S9\", \"S5\"],\n",
    "    \"S5\": [\"S4\", \"S9\", \"S14\"],\n",
    "    \"S6\": [\"S1\", \"S2\", \"S10\", \"S11\", \"S7\"],\n",
    "    \"S7\": [\"S2\", \"S3\", \"S8\", \"S12\", \"S11\", \"S6\"],\n",
    "    \"S8\": [\"S3\", \"S4\", \"S7\", \"S9\", \"S12\", \"S13\"],\n",
    "    \"S9\": [\"S4\", \"S5\", \"S8\", \"S13\", \"S14\"],\n",
    "    \"S10\": [\"S1\", \"S6\", \"S11\"],\n",
    "    \"S11\": [\"S10\", \"S6\", \"S2\", \"S7\", \"S12\"],\n",
    "    \"S12\": [\"S11\", \"S7\", \"S8\", \"S13\"],\n",
    "    \"S13\": [\"S12\", \"S8\", \"S9\", \"S14\"],\n",
    "    \"S14\": [\"S13\", \"S9\", \"S5\"]\n",
    "}\n",
    "\n",
    "################################################################################################\n",
    "# Add weather score to states \n",
    "################################################################################################\n",
    "\n",
    "# Weather exposure lookup dictionary\n",
    "weather_exposure = {\n",
    "    (1, 2): 95, (1, 6): 1, (1, 10): 1,\n",
    "    (2, 1): 95, (2, 6): 5, (2, 7): 90, (2, 3): 44, (2, 11): 32,\n",
    "    (3, 2): 44, (3, 7): 57, (3, 8): 9, (3, 4): 41,\n",
    "    (4, 3): 41, (4, 8): 54, (4, 9): 50, (4, 5): 65,\n",
    "    (5, 4): 65, (5, 9): 9, (5, 14): 52,\n",
    "    (6, 1): 1, (6, 2): 5, (6, 10): 27, (6, 11): 50, (6, 7): 82,\n",
    "    (7, 2): 90, (7, 3): 57, (7, 8): 51, (7, 12): 33, (7, 11): 27, (7, 6): 82,\n",
    "    (8, 3): 9, (8, 4): 54, (8, 7): 51, (8, 9): 84, (8, 12): 36, (8, 13): 22,\n",
    "    (9, 4): 50, (9, 5): 9, (9, 8): 84, (9, 13): 31, (9, 14): 39,\n",
    "    (10, 1): 1, (10, 6): 27, (10, 11): 52,\n",
    "    (11, 10): 52, (11, 6): 50, (11, 2): 32, (11, 7): 27, (11, 12): 18,\n",
    "    (12, 11): 18, (12, 7): 33, (12, 8): 36, (12, 13): 46,\n",
    "    (13, 12): 46, (13, 8): 22, (13, 9): 31, (13, 14): 80,\n",
    "    (14, 13): 80, (14, 9): 39, (14, 5): 52\n",
    "}\n",
    "\n",
    "# Assign weather exposure scores for each action\n",
    "weather_scores = {\n",
    "    state: {action: weather_exposure.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for weather score\n",
    "# define the state and action to retrieve the weather score\n",
    "state, action = \"S3\", \"S8\"\n",
    "weather_score = weather_scores[state][action]\n",
    "\n",
    "################################################################################################\n",
    "# Add safety score to states\n",
    "################################################################################################\n",
    "\n",
    "# Safety exposure lookup dictionary\n",
    "safety_exposure = {\n",
    "    (1, 2): 22, (1, 6): 13, (1, 10): 15,\n",
    "    (2, 1): 22, (2, 6): 19, (2, 7): 36, (2, 3): 41, (2,11): 21,\n",
    "    (3, 2): 41, (3, 7): 36, (3, 8): 42, (3, 4): 48,\n",
    "    (4, 3): 48, (4, 8): 42, (4, 9): 62, (4, 5): 66,\n",
    "    (5, 4): 66, (5, 9): 99, (5, 14): 99,\n",
    "    (6, 1): 13, (6, 2): 19, (6, 10): 0, (6, 11): 18, (6, 7): 21,\n",
    "    (7, 2): 36, (7, 3): 36, (7, 8): 38, (7, 12): 39, (7, 11): 18, (7, 6): 21,\n",
    "    (8, 3): 42, (8, 4): 42, (8, 7): 38, (8, 9): 64, (8, 12): 58, (8, 13): 69,\n",
    "    (9, 4): 62, (9, 5): 99, (9, 8): 64, (9, 13): 58, (9, 14): 99,\n",
    "    (10, 1): 15, (10, 6): 0, (10, 11): 1,\n",
    "    (11, 10): 1, (11, 6): 18, (11, 2): 21, (11, 7): 18, (11, 12): 46,\n",
    "    (12, 11): 46, (12, 7): 39, (12, 8): 58, (12, 13): 69,\n",
    "    (13, 12): 69, (13, 8): 69, (13, 9): 58, (13, 14): 99,\n",
    "    (14, 13): 99, (14, 9): 99, (14, 5): 99\n",
    "}\n",
    "\n",
    "# Assign safety scores for each action\n",
    "safety_scores = {\n",
    "    state: {action: safety_exposure.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for safety score\n",
    "# define the state and action to retrieve the safety score\n",
    "state, action = \"S3\", \"S8\"\n",
    "safety_score = safety_scores[state][action]\n",
    "\n",
    "################################################################################################\n",
    "# Add travel time score to states\n",
    "################################################################################################\n",
    "\n",
    "# Travel Time lookup dictionary\n",
    "travel_time = {\n",
    "    (1, 2): 20, (1, 6): 80, (1, 10): 100,\n",
    "    (2, 1): 20, (2, 6): 60, (2, 7): 60, (2, 3): 40, (2, 11): 100,\n",
    "    (3, 2): 40, (3, 7): 60, (3, 8): 60, (3, 4): 20,\n",
    "    (4, 3): 20, (4, 8): 60, (4, 9): 60, (4, 5): 20,\n",
    "    (5, 4): 20, (5, 9): 60, (5, 14): 100,\n",
    "    (6, 1): 80, (6, 2): 60, (6, 10): 60, (6, 11): 40, (6, 7): 20,\n",
    "    (7, 2): 60, (7, 3): 60, (7, 8): 20, (7, 12): 60, (7, 11): 60, (7, 6): 20,\n",
    "    (8, 3): 60, (8, 4): 60, (8, 7): 20, (8, 9): 20, (8, 12): 60, (8, 13): 40,\n",
    "    (9, 4): 60, (9, 5): 60, (9, 8): 20, (9, 13): 60, (9, 14): 60,\n",
    "    (10, 1): 100, (10, 6): 60, (10, 11): 20,\n",
    "    (11, 10): 20, (11, 6): 40, (11, 2): 100, (11, 7): 60, (11, 12): 20,\n",
    "    (12, 11): 20, (12, 7): 60, (12, 8): 60, (12, 13): 20,\n",
    "    (13, 12): 20, (13, 8): 40, (13, 9): 60, (13, 14): 20,\n",
    "    (14, 13): 20, (14, 9): 60, (14, 5): 100\n",
    "}\n",
    "\n",
    "# Assign travel time scores for each action\n",
    "travel_time_scores = {\n",
    "    state: {action: travel_time.get((int(state[1:]), int(action[1:])), 1000)\n",
    "            for action in neighbors}\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# O(1) lookup for travel time score\n",
    "# define the state and action to retrieve the travel time score\n",
    "state, action = \"S3\", \"S8\"\n",
    "travel_time_score = travel_time_scores[state][action]\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Function to dynamically calculate the reward based on the state and action, using the scores\n",
    "################################################################################################\n",
    "\n",
    "# Define the normalization ranges (assuming max and min possible values for each score type)\n",
    "WEATHER_MAX = 1000\n",
    "SAFETY_MAX = 1000\n",
    "TRAVEL_TIME_MAX = 1000\n",
    "\n",
    "# Define the weights for each score (adjust based on their importance)\n",
    "WEATHER_WEIGHT = 0.3\n",
    "SAFETY_WEIGHT = 0.4\n",
    "TRAVEL_TIME_WEIGHT = 0.3\n",
    "\n",
    "\n",
    "def normalize_score(score, max_score):\n",
    "    \"\"\" Normalize the score based on the maximum score for each category. \"\"\"\n",
    "    return score / max_score\n",
    "\n",
    "\n",
    "def calculate_reward(state, action):\n",
    "    \"\"\" \n",
    "    Smarter reward function that considers normalized scores and weighted importance. \n",
    "    Incorporates dynamic scaling for weather, safety, and travel time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve individual scores\n",
    "    weather_score = weather_scores[state][action]\n",
    "    safety_score = safety_scores[state][action]\n",
    "    travel_time_score = travel_time_scores[state][action]\n",
    "\n",
    "    # Normalize the scores to be between 0 and 1\n",
    "    normalized_weather = normalize_score(weather_score, WEATHER_MAX)\n",
    "    normalized_safety = normalize_score(safety_score, SAFETY_MAX)\n",
    "    normalized_travel_time = normalize_score(\n",
    "        travel_time_score, TRAVEL_TIME_MAX)\n",
    "\n",
    "    # Calculate weighted sum of normalized scores\n",
    "    total_score = (WEATHER_WEIGHT * normalized_weather +\n",
    "                   SAFETY_WEIGHT * normalized_safety +\n",
    "                   TRAVEL_TIME_WEIGHT * normalized_travel_time)\n",
    "\n",
    "    # Reward is the negative of the total score (since higher scores should be worse)\n",
    "    # You can adjust this if you prefer positive rewards for better states\n",
    "    reward = -1 * total_score\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "# Example usage: calculate reward for state \"S3\" and action \"S8\"\n",
    "state, action = \"S3\", \"S8\"\n",
    "reward = calculate_reward(state, action)\n",
    "print(f\"Reward for state {state} and action {action}: {reward}\")\n",
    "\n",
    "################################################################################################\n",
    "# Fill up the Rewards dictionary\n",
    "################################################################################################\n",
    "\n",
    "# Initialize the rewards dictionary using the same structure as actions\n",
    "rewards = {\n",
    "    state: [calculate_reward(state, action) for action in neighbors]\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# Print the rewards table in a readable format\n",
    "print(\"Rewards Table:\")\n",
    "print(\n",
    "    \"State -> [Action1, Action2, ..., ActionN] -> [Reward1, Reward2, ..., RewardN]\")\n",
    "\n",
    "for state, neighbors in actions.items():\n",
    "    rewards_for_state = rewards[state]\n",
    "    print(f\"{state} -> {neighbors} -> {rewards_for_state}\")\n",
    "\n",
    "################################################################################################\n",
    "# Implement transition probabilities\n",
    "################################################################################################\n",
    "\n",
    "# Define the transition probabilities for each state-action pair\n",
    "def generate_transition_probability(state, action, states):\n",
    "    # Assuming a random probability for demonstration (this can be customized)\n",
    "    prob_sum = 0\n",
    "    transition_probabilities = {}\n",
    "\n",
    "    # Assigning random probabilities to all possible transitions\n",
    "    for next_state in states:\n",
    "        if next_state != state:\n",
    "            # Generate a random probability between 0 and 1\n",
    "            prob = random.uniform(0, 1)\n",
    "            transition_probabilities[next_state] = prob\n",
    "            prob_sum += prob\n",
    "\n",
    "    # Normalize probabilities to ensure they sum up to 1\n",
    "    for next_state in transition_probabilities:\n",
    "        transition_probabilities[next_state] /= prob_sum\n",
    "\n",
    "    return transition_probabilities\n",
    "\n",
    "\n",
    "# Create transition probabilities for each state-action pair\n",
    "transition_probabilities = {\n",
    "    state: {\n",
    "        action: generate_transition_probability(state, action, actions.keys())\n",
    "        for action in neighbors\n",
    "    }\n",
    "    for state, neighbors in actions.items()\n",
    "}\n",
    "\n",
    "# Print the transition probabilities table for each state-action pair\n",
    "print(\"Transition Probabilities Table:\")\n",
    "for state, neighbors in actions.items():\n",
    "    for action in neighbors:\n",
    "        print(\n",
    "            f\"State {state}, Action {action} -> Transition Probabilities: {transition_probabilities[state][action]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and solve the MDP\n",
    "route_planner = RoutePlannerMDP(\n",
    "    states, actions, transition_probabilities, rewards)\n",
    "route_planner.value_iteration()\n",
    "\n",
    "# Find optimal route from S1\n",
    "optimal_route = route_planner.get_optimal_route(\"S1\")\n",
    "print(\"Optimal Route:\", optimal_route)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
